"""
Scalability Experiment for ICS Sensor Privacy System
í™•ì¥ì„± ì‹¤í—˜ - ì„¼ì„œ ìˆ˜ì™€ ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ ë¹„êµ
"""

import asyncio
import time
import json
import csv
import psutil
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, Any, List, Tuple
from datetime import datetime
import sys
from rich.console import Console
from rich.progress import Progress, TaskID, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.table import Table
from rich.layout import Layout
from rich.panel import Panel
from rich.live import Live

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from config.settings import ServerConfig, HAI_SENSORS, SWAT_SENSORS
from crypto.bulletproofs import BulletproofGenerator
from crypto.hmac_baseline import HMACBaseline
from crypto.ed25519_baseline import Ed25519Baseline
from sensors.multi_sensor import MultiSensorSimulator


class ScalabilityExperiment:
    """
    í™•ì¥ì„± ì‹¤í—˜ í´ë˜ìŠ¤
    ì„¼ì„œ ìˆ˜ì™€ ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ ì¸¡ì •
    """
    
    def __init__(self, server_config: ServerConfig = None):
        """
        í™•ì¥ì„± ì‹¤í—˜ ì´ˆê¸°í™”
        
        Args:
            server_config: ì„œë²„ ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        self.sensor_counts = [1, 10, 25, 50, 100]
        self.algorithms = ['bulletproofs', 'hmac', 'ed25519']
        self.server_config = server_config or ServerConfig()
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.results_dir = Path(__file__).parent.parent / "results"
        self.results_dir.mkdir(exist_ok=True)
        
        # ì‹¤í—˜ ê²°ê³¼ ì €ì¥
        self.experiment_results = []
        self.metrics_history = []
        
        # ì½˜ì†” ì¶œë ¥ìš©
        self.console = Console()
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ êµ¬í˜„ì²´ ì´ˆê¸°í™”
        self.crypto_instances = {
            'bulletproofs': BulletproofGenerator(bit_length=16),  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
            'hmac': HMACBaseline(),
            'ed25519': Ed25519Baseline()
        }
        
        # ì„¼ì„œ ì„¤ì •
        self.all_sensors = {**HAI_SENSORS, **SWAT_SENSORS}
        self.sensor_list = list(self.all_sensors.keys())
        
    async def test_algorithm(self, algorithm: str, num_sensors: int, duration: int = 60) -> Dict[str, Any]:
        """
        íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ê³¼ ì„¼ì„œ ìˆ˜ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        
        Args:
            algorithm: í…ŒìŠ¤íŠ¸í•  ì•Œê³ ë¦¬ì¦˜
            num_sensors: ì„¼ì„œ ê°œìˆ˜
            duration: í…ŒìŠ¤íŠ¸ ì§€ì† ì‹œê°„ (ì´ˆ)
            
        Returns:
            í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        self.console.print(f"[cyan]Testing {algorithm} with {num_sensors} sensors for {duration}s[/cyan]")
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        start_time = time.time()
        initial_cpu = psutil.cpu_percent(interval=1)
        initial_memory = psutil.virtual_memory()
        
        # ì„¼ì„œ ì„ íƒ (ìˆœí™˜í•˜ì—¬ ì‚¬ìš©)
        selected_sensors = []
        for i in range(num_sensors):
            sensor_id = self.sensor_list[i % len(self.sensor_list)]
            selected_sensors.append(sensor_id)
        
        # ë©€í‹°ì„¼ì„œ ì‹œë®¬ë ˆì´í„° ìƒì„±
        simulator = MultiSensorSimulator(
            sensor_configs=[self.all_sensors[sid] for sid in selected_sensors],
            server_config=self.server_config,
            algorithm=algorithm
        )
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì„ ìœ„í•œ íƒœìŠ¤í¬
        metrics_data = []
        async def collect_metrics():
            """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
            while True:
                try:
                    cpu_percent = psutil.cpu_percent(interval=0.5)
                    memory = psutil.virtual_memory()
                    timestamp = time.time()
                    
                    metrics_data.append({
                        'timestamp': timestamp,
                        'cpu_percent': cpu_percent,
                        'memory_used_mb': memory.used / (1024 * 1024),
                        'memory_percent': memory.percent
                    })
                    
                    await asyncio.sleep(1)
                except asyncio.CancelledError:
                    break
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ íƒœìŠ¤í¬ ì‹œì‘
        metrics_task = asyncio.create_task(collect_metrics())
        
        try:
            # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
            simulation_results = await simulator.run(duration)
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ë‹¨
            metrics_task.cancel()
            
            # ìµœì¢… ì‹œìŠ¤í…œ ìƒíƒœ
            end_time = time.time()
            final_cpu = psutil.cpu_percent(interval=1)
            final_memory = psutil.virtual_memory()
            
            # ê²°ê³¼ ì§‘ê³„
            total_duration = end_time - start_time
            samples_sent = simulation_results.get('total_samples_sent', 0)
            total_errors = simulation_results.get('total_errors', 0)
            
            # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
            throughput = samples_sent / total_duration if total_duration > 0 else 0
            error_rate = (total_errors / samples_sent * 100) if samples_sent > 0 else 0
            
            # CPU/ë©”ëª¨ë¦¬ í†µê³„
            if metrics_data:
                cpu_usage = [m['cpu_percent'] for m in metrics_data]
                memory_usage = [m['memory_used_mb'] for m in metrics_data]
                
                avg_cpu = np.mean(cpu_usage)
                max_cpu = np.max(cpu_usage)
                avg_memory = np.mean(memory_usage)
                max_memory = np.max(memory_usage)
            else:
                avg_cpu = max_cpu = (initial_cpu + final_cpu) / 2
                avg_memory = max_memory = initial_memory.used / (1024 * 1024)
            
            # ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥ ë°ì´í„°
            crypto_instance = self.crypto_instances[algorithm]
            if hasattr(crypto_instance, 'measure_performance'):
                crypto_perf = crypto_instance.measure_performance(100)
                avg_generation_time = crypto_perf.get('avg_generation_time_ms', 0)
                avg_verification_time = crypto_perf.get('avg_verification_time_ms', 0)
                data_size = crypto_perf.get('total_data_size_bytes', 0)
            else:
                avg_generation_time = avg_verification_time = data_size = 0
            
            result = {
                'algorithm': algorithm,
                'num_sensors': num_sensors,
                'duration': duration,
                'total_duration': total_duration,
                'samples_sent': samples_sent,
                'total_errors': total_errors,
                'throughput_samples_per_sec': throughput,
                'error_rate_percent': error_rate,
                'avg_cpu_percent': avg_cpu,
                'max_cpu_percent': max_cpu,
                'avg_memory_mb': avg_memory,
                'max_memory_mb': max_memory,
                'avg_generation_time_ms': avg_generation_time,
                'avg_verification_time_ms': avg_verification_time,
                'data_size_bytes': data_size,
                'timestamp': datetime.now().isoformat(),
                'metrics_samples': len(metrics_data)
            }
            
            self.console.print(f"[green]âœ“ {algorithm} with {num_sensors} sensors completed[/green]")
            self.console.print(f"  Throughput: {throughput:.2f} samples/sec")
            self.console.print(f"  Avg CPU: {avg_cpu:.1f}%, Max Memory: {max_memory:.1f}MB")
            
            return result
            
        except Exception as e:
            metrics_task.cancel()
            self.console.print(f"[red]âœ— Error testing {algorithm} with {num_sensors} sensors: {e}[/red]")
            return {
                'algorithm': algorithm,
                'num_sensors': num_sensors,
                'duration': duration,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    async def run(self, test_duration: int = 60) -> Dict[str, Any]:
        """
        ì „ì²´ í™•ì¥ì„± ì‹¤í—˜ ì‹¤í–‰
        
        Args:
            test_duration: ê° í…ŒìŠ¤íŠ¸ì˜ ì§€ì† ì‹œê°„ (ì´ˆ)
            
        Returns:
            ì „ì²´ ì‹¤í—˜ ê²°ê³¼
        """
        self.console.print("[bold blue]ğŸš€ ICS Sensor Privacy Scalability Experiment Started[/bold blue]")
        self.console.print(f"Server: {self.server_config.url}")
        self.console.print(f"Algorithms: {', '.join(self.algorithms)}")
        self.console.print(f"Sensor counts: {self.sensor_counts}")
        self.console.print(f"Test duration per configuration: {test_duration}s")
        self.console.print("-" * 60)
        
        # ì´ í…ŒìŠ¤íŠ¸ ìˆ˜ ê³„ì‚°
        total_tests = len(self.algorithms) * len(self.sensor_counts)
        
        # ì§„í–‰ìƒí™© í‘œì‹œ
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
            console=self.console
        ) as progress:
            
            main_task = progress.add_task("Overall Progress", total=total_tests)
            
            # ê° ì•Œê³ ë¦¬ì¦˜ë³„ë¡œ ì‹¤í—˜ ì‹¤í–‰
            for algorithm in self.algorithms:
                algorithm_results = []
                
                # ê° ì„¼ì„œ ìˆ˜ë³„ë¡œ í…ŒìŠ¤íŠ¸
                for sensor_count in self.sensor_counts:
                    test_desc = f"Testing {algorithm} ({sensor_count} sensors)"
                    test_task = progress.add_task(test_desc, total=1)
                    
                    try:
                        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                        result = await self.test_algorithm(algorithm, sensor_count, test_duration)
                        algorithm_results.append(result)
                        self.experiment_results.append(result)
                        
                        progress.update(test_task, advance=1)
                        progress.update(main_task, advance=1)
                        
                        # ì ì‹œ ëŒ€ê¸° (ì‹œìŠ¤í…œ ì•ˆì •í™”)
                        await asyncio.sleep(2)
                        
                    except Exception as e:
                        self.console.print(f"[red]Error in {test_desc}: {e}[/red]")
                        progress.update(test_task, advance=1)
                        progress.update(main_task, advance=1)
                
                self.console.print(f"[cyan]Completed all tests for {algorithm}[/cyan]")
        
        # ê²°ê³¼ ì €ì¥
        await self.save_results()
        
        # ê²°ê³¼ í‘œì‹œ
        self.display_results_table()
        
        # ê·¸ë˜í”„ ìƒì„±
        self.plot_results()
        
        experiment_summary = {
            'total_tests': total_tests,
            'completed_tests': len(self.experiment_results),
            'algorithms_tested': self.algorithms,
            'sensor_counts_tested': self.sensor_counts,
            'test_duration_per_config': test_duration,
            'results_saved_to': str(self.results_dir),
            'timestamp': datetime.now().isoformat()
        }
        
        self.console.print("[bold green]âœ… Scalability Experiment Completed![/bold green]")
        return experiment_summary
    
    def collect_metrics(self) -> Dict[str, Any]:
        """
        í˜„ì¬ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        
        Returns:
            ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        metrics = {
            'timestamp': time.time(),
            'cpu_percent': cpu_percent,
            'memory_used_mb': memory.used / (1024 * 1024),
            'memory_available_mb': memory.available / (1024 * 1024),
            'memory_percent': memory.percent
        }
        
        self.metrics_history.append(metrics)
        return metrics
    
    async def save_results(self) -> None:
        """
        ì‹¤í—˜ ê²°ê³¼ë¥¼ JSON ë° CSV íŒŒì¼ë¡œ ì €ì¥
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ì €ì¥
        json_file = self.results_dir / f"scalability_results_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'experiment_config': {
                    'sensor_counts': self.sensor_counts,
                    'algorithms': self.algorithms,
                    'server_config': self.server_config.__dict__
                },
                'results': self.experiment_results,
                'metrics_history': self.metrics_history
            }, f, indent=2, ensure_ascii=False)
        
        # CSV ì €ì¥ (ë¶„ì„ìš©)
        if self.experiment_results:
            csv_file = self.results_dir / f"scalability_results_{timestamp}.csv"
            df = pd.DataFrame(self.experiment_results)
            df.to_csv(csv_file, index=False, encoding='utf-8')
        
        self.console.print(f"[green]Results saved to:[/green]")
        self.console.print(f"  JSON: {json_file}")
        if self.experiment_results:
            self.console.print(f"  CSV: {csv_file}")
    
    def display_results_table(self) -> None:
        """
        ì‹¤í—˜ ê²°ê³¼ë¥¼ í…Œì´ë¸” í˜•íƒœë¡œ í‘œì‹œ
        """
        if not self.experiment_results:
            self.console.print("[yellow]No results to display[/yellow]")
            return
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ë¡œ ê·¸ë£¹í™”
        for algorithm in self.algorithms:
            algo_results = [r for r in self.experiment_results if r.get('algorithm') == algorithm]
            
            if not algo_results:
                continue
            
            table = Table(title=f"{algorithm.upper()} Performance Results")
            table.add_column("Sensors", style="cyan")
            table.add_column("Throughput\n(samples/sec)", style="green")
            table.add_column("Avg CPU\n(%)", style="yellow")
            table.add_column("Max Memory\n(MB)", style="red")
            table.add_column("Gen Time\n(ms)", style="blue")
            table.add_column("Verify Time\n(ms)", style="magenta")
            table.add_column("Error Rate\n(%)", style="red")
            
            for result in sorted(algo_results, key=lambda x: x.get('num_sensors', 0)):
                if 'error' in result:
                    continue
                    
                table.add_row(
                    str(result.get('num_sensors', 0)),
                    f"{result.get('throughput_samples_per_sec', 0):.2f}",
                    f"{result.get('avg_cpu_percent', 0):.1f}",
                    f"{result.get('max_memory_mb', 0):.1f}",
                    f"{result.get('avg_generation_time_ms', 0):.3f}",
                    f"{result.get('avg_verification_time_ms', 0):.3f}",
                    f"{result.get('error_rate_percent', 0):.2f}"
                )
            
            self.console.print(table)
            self.console.print()
    
    def plot_results(self) -> None:
        """
        í™•ì¥ì„± ê·¸ë˜í”„ ìƒì„± ë° ì €ì¥
        """
        if not self.experiment_results:
            self.console.print("[yellow]No results to plot[/yellow]")
            return
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame([r for r in self.experiment_results if 'error' not in r])
        
        if df.empty:
            self.console.print("[yellow]No valid results to plot[/yellow]")
            return
        
        # ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ ì„¤ì •
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # ì„œë¸Œí”Œë¡¯ ìƒì„±
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ICS Sensor Privacy System - Scalability Analysis', fontsize=16, fontweight='bold')
        
        # 1. ì²˜ë¦¬ëŸ‰ vs ì„¼ì„œ ìˆ˜
        ax1 = axes[0, 0]
        for algorithm in self.algorithms:
            algo_data = df[df['algorithm'] == algorithm]
            if not algo_data.empty:
                ax1.plot(algo_data['num_sensors'], algo_data['throughput_samples_per_sec'], 
                        marker='o', linewidth=2, markersize=6, label=algorithm.upper())
        
        ax1.set_xlabel('Number of Sensors')
        ax1.set_ylabel('Throughput (samples/sec)')
        ax1.set_title('Throughput vs Number of Sensors')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. CPU ì‚¬ìš©ë¥  vs ì„¼ì„œ ìˆ˜
        ax2 = axes[0, 1]
        for algorithm in self.algorithms:
            algo_data = df[df['algorithm'] == algorithm]
            if not algo_data.empty:
                ax2.plot(algo_data['num_sensors'], algo_data['avg_cpu_percent'], 
                        marker='s', linewidth=2, markersize=6, label=algorithm.upper())
        
        ax2.set_xlabel('Number of Sensors')
        ax2.set_ylabel('Average CPU Usage (%)')
        ax2.set_title('CPU Usage vs Number of Sensors')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ vs ì„¼ì„œ ìˆ˜
        ax3 = axes[1, 0]
        for algorithm in self.algorithms:
            algo_data = df[df['algorithm'] == algorithm]
            if not algo_data.empty:
                ax3.plot(algo_data['num_sensors'], algo_data['max_memory_mb'], 
                        marker='^', linewidth=2, markersize=6, label=algorithm.upper())
        
        ax3.set_xlabel('Number of Sensors')
        ax3.set_ylabel('Max Memory Usage (MB)')
        ax3.set_title('Memory Usage vs Number of Sensors')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ì²˜ë¦¬ ì‹œê°„ ë¹„êµ (ìƒì„± + ê²€ì¦)
        ax4 = axes[1, 1]
        for algorithm in self.algorithms:
            algo_data = df[df['algorithm'] == algorithm]
            if not algo_data.empty:
                total_time = algo_data['avg_generation_time_ms'] + algo_data['avg_verification_time_ms']
                ax4.plot(algo_data['num_sensors'], total_time, 
                        marker='d', linewidth=2, markersize=6, label=algorithm.upper())
        
        ax4.set_xlabel('Number of Sensors')
        ax4.set_ylabel('Total Processing Time (ms)')
        ax4.set_title('Total Processing Time vs Number of Sensors')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_yscale('log')  # ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ì°¨ì´ ëª…í™•íˆ í‘œì‹œ
        
        plt.tight_layout()
        
        # ê·¸ë˜í”„ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_file = self.results_dir / f"scalability_plots_{timestamp}.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.show()
        
        self.console.print(f"[green]Scalability plots saved to: {plot_file}[/green]")


# ì‹¤í–‰ ì˜ˆì œ
if __name__ == "__main__":
    async def main():
        print("ğŸ”¬ ICS Sensor Privacy Scalability Experiment")
        print("=" * 60)
        
        # ì„œë²„ ì„¤ì •
        server_config = ServerConfig(host="localhost", port=8084)
        
        # ì‹¤í—˜ ìƒì„±
        experiment = ScalabilityExperiment(server_config)
        
        try:
            # ì‹¤í—˜ ì‹¤í–‰ (ê° ì„¤ì •ë‹¹ 30ì´ˆì”© í…ŒìŠ¤íŠ¸)
            results = await experiment.run(test_duration=30)
            
            print(f"\nâœ… Experiment completed successfully!")
            print(f"Total tests: {results['completed_tests']}/{results['total_tests']}")
            print(f"Results saved to: {results['results_saved_to']}")
            
        except KeyboardInterrupt:
            print("\nâš ï¸ Experiment interrupted by user")
        except Exception as e:
            print(f"\nâŒ Experiment failed: {e}")
    
    # ì‹¤í—˜ ì‹¤í–‰
    asyncio.run(main())